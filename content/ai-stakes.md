---
title: AI's Quest for Bias-Free Enlightenment
date: 2024-02-23
---

This week, Google's newly released Gemini product showed some crazy innacuracies. When generating images of historical figures, like the Founding Fathers, [Gemini assigned incorrect races and genders](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical).

It's a reminder that AI is shaped only by its training data, and reflects the biases of the engineers who curate that data. Google's engineering team chose training dat, influenced by their own biases and blind spots, which led to some strange historical hallucinations.

**Engineering Human Values**

Google, OpenAI, Meta, etc. are now challenged with the impossible: balance historical accuracy with cultural sensitivity. The Founding Fathers example is an easy one to fix. Generate things based on things that are generated before. The Founding Fathers were white. So make them white. But it's really not so easy.

When I asked Gemini for an image of "Manifest Destiny," it opted for a Wikipedia image, sidestepping all responsibility. But even the Wikipedia image is a controversial one. It focuses on Columbia, a personification of America, leading the way west. Somewhere in there you can also see Native Americans fading into the darkness, but it's easy to miss, and hardly captures their bloodshed, pain, and suffering.

![manifest destiny](/images/American_Progress_(John_Gast_painting).jpg)

How can these companies embed universal values into products like these without triggering cultural controversy? 

The answer is pretty clear.

They can't.