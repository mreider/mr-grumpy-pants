---
title: AI's Quest for Bias-Free Enlightenment
date: 2024-02-23
---

This week, Google's newly released Gemini product demonstrated some notable inaccuracies. When generating images of historical figures like George Washington, Gemini [sometimes depicted them with incorrect races or genders](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical).

<img src="/images/wash.png" alt="Manifest Destiny" style="width:50%;height: auto;">

Asked why it altered Washington's race, Gemini explained that it "aimed to provide a more accurate and inclusive representation..."  Perhaps, like a Dr. Strange movie, an alternate timeline in which Washington is black might be for the best.

Preferred historical depictions aside, Gemini's behavior reminds us that AI is shaped by training data that reflects the biases of the engineers who curate it. In this case, Google's team made choices that influenced those results.

**Engineering Human Values**

Today, when I asked Gemini to generate an image depicting "Manifest Destiny," it provided a Wikipedia image instead. That's because Google has temporarily disabled image generation for sensitive prompts. But let's imagine for a moment that Gemini generated the image on its own.

<img src="/images/American_Progress_(John_Gast_painting).jpg" alt="Manifest Destiny" style="width:50%;height: auto;">

The picture shows Columbia, a symbol of America, leading the way West. Native Americans are barely noticeable; their figures fade into the background, easy to overlook. This kind of imagery reflects a specific viewpoint, one that may or may not align with your own.  As a Gemini user, your product satisfaction depends on how closely these values mirror those of the engineers who trained the AI. This means that universal approval for Google Gemini is unlikely, no matter how hard they try.

Tech giants like Google, OpenAI, and Meta will never be able to create a generally acceptable form of AI-generated "truth." Of course, that's impossible. But that won't stop them from trying. Perhaps users will have settings to tailor the political bias of the answers they receive, ensuring they reflect their own worldviews.  More likely, we'll see a proliferation of AI engines targeted at specific audiences, furthering the divide we already see in the media.